\chapter{Manipulieren von Bildern}\label{chp:bildmanipulation} %10 Seiten
\glsresetall

 Wie aus Kapitel \ref{chp:bildgenerierung} festzustellen ist, können gute Landschaftsbilder erzeugt werden. Deshalb folgt nun der nächste Schritt, das manipulieren der Bilder. Das Ziel ist es ein Landschaftsbilder zu verschiedenen Tageszeit zu haben, dafür soll der Inhalt der Eingabe beibehalten werden,  der Stile des Bildes sich aber der Tageszeit anpassen. Zum Beispiel aus einem Bild am Tag soll ein Bild bei Nacht werden.
 
 \section{Modelle} % Dani
 Für das manipulieren von Bilder gibt es verschieden Ansätze. Eingegrenzt wurde diese durch den Datensatz, da die Bilder nicht in Paaren vorliegen, es also zu einem Eingabebild keine entsprechendes Bild in der Zieldomain gibt und das Verfahren unsupervised sein soll. %vllt warum unsupervised?
 In Rahmen dieser Arbeit wurden sich mit  zwei Ansätze genauer beschäftigt. Der erste Ansatz ist der des CycleGANs \cite{zhu2017unpaired}, welches mittels des cycle consistency losses versucht Bilder zwischen zwei Domänen zu transferieren. Der zweite Ansatz heißt  \gls{acr-funit} \cite{liu2019few} der sich am Vorgehen des Menschen Gehirns orientiert und versucht mit wenigen Bilder aus ähnlichen Domänen die Übersetzung zu lernen.
 
 \subsection{CycleGAN}% Dani
 \label{sub:cyclegan}
 Durch das Fehlen von gepaarten Bilder fehlt eine Möglichkeit zu überprüfen, ob der Generator den Inhalt des Bildes bei behält. \citeauthor{zhu2017unpaired} \cite{zhu2017unpaired} lösen diese Problem durch einfügen einer weiteren Bedingung: Bilder sollen, wenn sie übersetzt wurden und anschließen wieder zurück umgewandelt werden, gleich sein bzw. sehr ähnlich. Dies wird mittels des cycle consistency loss umgesetzt. 
 Dazu benötigt man zwei GANs, eins welches aus der Eingabedomäne in die Zieldomäne übersetzt, $G: X \rightarrow Y$ und ein anderes welches das Gegenteil durchführt,   $F: Y \rightarrow X$.
 Der cycle consistency loss gibt an wie groß der Verlust zwischen dem ursprünglichen Bild und der Ausgabe nach dem manipulieren beider GANs. 
 Das Ziel ist es $F(G(X))\approx X$ sowie $G(F(Y))\approx Y$ durchzusetzen. Wie in Abbildung \ref{fig:cylce_loss} zusehen, wobei der Verlust jeweils zwischen $x$, $\hat{x}$ und $y$,$\hat{y}$ berechnet wird.
 Die jeweiligen GANs lernen dazu noch über den adversarial loss, also dem Feedback des jeweiligen Discriminators des GANs. 
 Dadurch ergibt sich der Verlust des CycleGANs als
 \[ \mathcal{L}(G,F,D_X,D_Y) = 	\mathcal{L}_{GAN}(G,D_Y,X,Y) + 	\mathcal{L}_{GAN}(F, D_X,Y,X) +\lambda 	\mathcal{L}_{cyc}(G,F)   \]
 wobei $D_X$ und $D_Y$ die Diskriminatoren für die jeweilige Domäne sind und  $\lambda$ die Wichtigkeit des cycle consistency loss festlegt. 
 Das Zeil des Lernens ist es 
 \[G^*,F^* = arg \min_{G,F} \max_{D_X,D_Y} \mathcal{L}(G,F,D_X,D_Y) \] zu lösen.
 
 \begin{figure}
 
 	\tikzstyle{block} = [draw=black, thick, text width=1cm, minimum height=1cm, align=center]  
 	\tikzstyle{line} = [draw=black, thick, text width=1cm, minimum height=3cm, align=center]  
 	\begin{tikzpicture}
 		
 		\node[block] (a) {$x$};
 		\node[block, right=of a] (b) {$\hat{Y}$};
 		\node[block, right=of b] (c) {$\hat{x}$};
 		\node (Dy) [above of=b] {$D_Y$};
 		
 		\node[block, right=of c] (d) {$y$};
 		\node[block, right=of d] (e) {$\hat{X}$};
 		\node[block, right=of e] (f) {$\hat{y}$};
 		\node (Dx) [above of=e] {$D_X$};
 		
 		\path [thick,->,>=stealth]
 		(a) edge [bend left] node [above] {$G$} (b)
 		(b) edge [bend right] node [above] {$F$} (c)
 		(d) edge [bend right] node [above] {$F$} (e)
 		(e) edge [bend left] node [above] {$G$} (f);
 		
 		\draw[dashed,->] (b) to node [] {} (Dy);
 		\draw[dashed,->] (e) to node [] {} (Dx);
 		
 		\draw ([yshift=0.7cm]$(c.north east)!.5!(d.north west)$) --([yshift=-0.7cm]$(c.south east)!.5!(d.south west)$);
 	\end{tikzpicture}
 	\caption[cycle consistency loss]{cycle consistency loss in beide Richtungen}
 	\label{fig:cylce_loss}
 \end{figure}
 
 Die Architektur der genutzten Netze stammt von \citeauthor{johnson_perceptual_2016} \cite{johnson_perceptual_2016}. Für den Generator gibt es zwei Varianten, die eine für Eingabegröße $128 \times 128$ mit 6 Residual Blöcken und die andere für Eingabegröße $256 \times 256$ mit 9 Residual Blöcken (ResBlock), diese Blöcke führen zwei Convolution mit gleicher Anzahl an Filter aus. Als Normalisierung wird Instance Normalisierung genutzt, die ist eine Erweiterung der Batch Normalisierung \cite{ulyanov_instance_2016}. In \cref{tab:cycleGAN} ist der Aufbau der Generators und der Discriminatoren für die Variante mit Eingabegröße $256 \times 256$ zusehen.
 
 \begin{table}[]
 	\caption{CycleGAN Architektur}
 	\label{tab:cycleGAN}
 	\begin{center}
 		\begin{minipage}{.5\linewidth}
 			\caption{CycleGAN Discriminator}
 			\centering
 			\begin{tabular}{lcl}
 				\toprule
 				Schicht & Kernel & Filter\\
 				\toprule
 				Conv, lReLU & $[4,4,2]$ & $64$\\
 				\midrule
 				Conv, IN, lReLU & $[4,4,2]$ & $128$\\
 				\midrule
 				Conv, IN, lReLU & $[4,4,2]$ & $256$\\
 				\midrule
 				Conv, IN, lReLU & $[4,4,2]$ & $512$\\
 				\midrule
 				Conv & &$1$\\
 				\bottomrule
 			\end{tabular}
 		\end{minipage}%
 		\begin{minipage}{.5\linewidth}
 			\centering
 			\caption{CycleGAN Generator}
 			\begin{tabular}{lcl}
 				\toprule
 				Schicht & Kernel & Filter\\
 				\toprule
 				Conv, IN, ReLU & $[7,7,1]$ & $64$\\
 				\midrule
 				Conv, IN, ReLU & $[3,3,2]$ & $128$\\
 				\midrule
 				Conv, IN, ReLU & $[3,3,2]$ & $256$\\
 				\midrule
 				ResBlock & $[3,3,1]$& $256$ \\
 				\midrule
 				ResBlock & $[3,3,1]$& $256$ \\
 				\midrule
 				ResBlock & $[3,3,1]$& $256$ \\
 				\midrule
 				ResBlock & $[3,3,1]$& $256$ \\
 				\midrule
 				ResBlock & $[3,3,1]$& $256$ \\
 				\midrule
 				ResBlock & $[3,3,1]$& $256$ \\
 				\midrule
 				ResBlock & $[3,3,1]$& $256$ \\
 				\midrule
 				ResBlock & $[3,3,1]$& $256$ \\
 				\midrule
 				ResBlock & $[3,3,1]$& $256$ \\
 				\midrule
 				DeConv, IN, ReLU & $[3,3,2]$ & $128$\\
 				\midrule
 				DeConv, IN, ReLU & $[3,3,2]$ & $64$\\
 				\midrule
 				Conv, IN, ReLU & $[7,7,1]$ & $3$\\
 				\bottomrule
 			\end{tabular}
 		\end{minipage} 
 	\end{center}
 	\begin{center}
 		\bigskip
 		\emph{Quelle:} Von \cite{zhu2017unpaired} übernommen.\\
 		\emph{Legende:} Der Kernel ist beschrieben im Format $[\text{x\_Größe},
 		\text{y\_Größe}, \text{Schrittweite}]$;IN steht für InstanceNorm; LeakyReLu hat eine Neigung von $0,2$;
 	\end{center}
 \end{table}
 
 
 Durch Evaluierungen und Vergleiche konnten \citeauthor{zhu2017unpaired} beobachten, dass die wiederhergestellten Bilder und die echten Bilder ein gute Ähnlichkeit aufwiesen. Vor allem bei Manipulationen bei denen Farben und Texturen geändert werden. Genau diese Veränderungen sollen in dieser Arbeit mit Landschaftsbilder durchgeführt werden. 
 
 \subsection{FUNIT}% Dani
 \label{sub:funit}
 Der Ansatz von \citeauthor{liu2019few} \cite{liu2019few}  geht das Problem an, das es oft nicht genug Bilder in der Eingabe- und Zielklassen gibt. Es wurde ein Ansatz mit wenigen Versuche (few-shot) entwickelt. Hierbei wird das Modell mit Bilder aus verschieden Eingabeklassen trainiert und erst beim Testen werden die Bilder der Zielklasse genutzt.
 Dazu wurde eine neues Netzwerkdesign mit dem gegnerischen Training von GANs vereinigt, der Name diese Modells ist  \acrfull{acr-funit}. Dieses Netzwerk besteht aus insgesamt vier Netzen, drei im Generator und eins im Discriminator. Der Generator $G$ bekommt als Eingabe ein Bild $x$ aus der Klasse $C_x$ und ein paar Bilder $y_1,\dots,y_n$ aus einer anderen Eingabeklasse $C_y$. Die Ausgabe $\hat{x}$ sollte dann den Inhalt von Bild $x$ in dem Aussehen der Klasse $K_y$ sein. 
 Die drei Netze die zusammen den Generator ergeben heißen Inhalt Encoder$E_x$, Klassen Encoder $E_y$ und Decoder $F_x$.
 Der Inhalt des Eingabebilds $x$ wird durch den Inhalt Encoder in eine räumliche Feature-Map umgewandelt. Die andern Eingabebilder also  $y_1,\dots,y_n$ werden in den Klassen Encoder gegeben, der  jedes Bild auf eine latenten mittel Vektor ( intermediate latent vector) abbildet und anschließend den Mittelwert über alle Vektoren berechnet, zu sehen in Abbildung \ref{fig:funitmodelsimple}. 
 Mit den Ausgaben aus den beiden Encoder konstruiert der Decoder das Ausgabebild des Generators. 
 \[\hat{x} = G(x, y_1,\dots,y_n)= F_x(E_x(x), E_y(y_1,\dots,y_n))\]
 
 Der Discriminator $D$ hat für jede Klasse die Aufgabe zu entscheiden, ob ein Bild echt ist oder durch den Generator erzeugt wurde. Es wird nur bestraft, wenn bei einem echten Bild nicht die richtige Klasse erkannt wird und wenn ein generierte Bild der Zielklasse zugeordnet wird. Der Generator bekommt nur dann eine Bestrafung, wenn das generiert Bilder nicht der gewünschten Klasse zugeordnet wurde.
 
 Beim Lernen wird das lösen folgender Gleichung versucht:
 
 \[\min_{D} \max_{G} \mathcal{L}_{GAN}(D,G) + \lambda_R \mathcal{L}_R(G) + \lambda_F \mathcal{L}_{F}(G) \]
 
 wobei $ \mathcal{L}_{GAN}(D,G)$ für den Loss des gesamten GANs,$ \mathcal{L}_R(G)$ den der Bildinhaltswiederherstellung und $\mathcal{L}_{F}(G) $ den der Feature-Abstimmung  steht. 
 
 Bei vergleichen mit anderen gängigen Ansätzen schnitt  \gls{acr-funit}  besser ab und benötigt dabei eine kleineren Datensatz \cite{liu2019few}. Diese beiden Vorteile bieten sich für die Manipulation die in dieser Arbeit durchgeführt werden soll an.
 
 \begin{figure}[h]
 	\centering
 	\includegraphics[width=0.8\linewidth]{images/Funit_Model_simple}
 	\caption[Modell FUNIT]{Modell des Generator von FUNIT}
 	\label{fig:funitmodelsimple}
 \end{figure}
 

 
 \section{Implementierung} % Tim nach Implementierung Generieren
  Für die Implementierung wurde sich auf Grund des Datensatzes und der beschränken Rechenleistung für das CycleGAN entschieden. Dieser wurde mit Tensorflow und Keras umgesetzt. Da dieses GAN deutlich größer als das \gls{acr-SNDCGAN} ist, konnte es, aufgrund der beschränkten Computerressourcen, nur einmal trainiert werden. Alle Trainingsparameter wurden vorher festgelegt, wobei die Erfahrung aus \cref{sec:gen_impl} genutzt wurden.
 
 \subsection{Umsetzung in Tensorflow/Keras}
 Wie auch für das \gls{acr-SNDCGAN} wurde ein \enquote{Sequential model} von Keras~\cite{keras:SequentialModel} verwendet um die einzelnen Schichten abzubilden. Da es nicht alle benötigten Netzschichten in Keras  implementiert sind mussten zudem für die ResBlock und ReflectionPadding2D Schichten, siehe \cref{sub:cyclegan}, sowie für Tanh,  eigene Klassen geschrieben werden, welche von einer abstrakten Keras Schicht erben. 
 
 Das Training des CycleGAN wurde ähnlich wie das \gls{acr-SNDCGAN} in \cref{sec:gen_impl} implementiert. Ebenfalls wurde es durch Checkpoints ermöglicht das Netz jederzeit anzuhalten und später weiter zu trainieren. Adam \cite{tf:adam} wurde als Optimizer, und GradientTapes \cite{tf:gradientape} zur Berechnung der Gradienten, verwendet.
 
Die Optimizer sind identisch parametrisiert und wurden wie in der Implementierung von \cite{brownlee_how_2019-1} auf eine Lernrate von 2e-4 und beta1 auf 0.5 gesetzt. 

Da das CycleGAN aus vier mehrschichtigen Netzen besteht, welche zusammen trainiert werden ist es sehr speicherintensiv zu trainieren. Nur dadurch, dass einzelne Trainingsschritte in eine Funktion mit dem \enquote{tf.function} decorater \cite{noauthor_tffunction_nodate}, ausgelagert sind kann die stärkste dem Projekt zur Verfügung stehende Grafikkarte genug speicher bieten, um das Training mit einer geringen Batchgröße durchzuführen. 


 \section{Evaluierung} % TODO Tim und Joshua

 \paragraph{Perceptual Distance} Als Metrik für die Evaluierung der
 Bildmanipulation wird eine \emph{\gls{acr-PD}} \cite{pang2021image} verwendet, die aus dem
 \emph{Perceptual Feature Reconstruction Loss} von
 \citeauthor{johnson_perceptual_2016} \cite{johnson_perceptual_2016} abgeleitet ist. Dazu wird, ähnlich der
 \gls{acr-FID}, die Aktivierung einer versteckten Schicht eines
 Bildklassifizierungsnetzes verglichen. In diesem Fall wird die 16-Schichten
 Variante des \emph{VGG} Netzes \cite{simonyan2014very} verwendet, welches auf
 dem ImageNet \cite{russakovsky2015imagenet} Datensatz trainiert wurde. Von diesem
 wird konkret die Aktivierung der drittletzten Convolution Schicht betrachtet.
 Die \gls{acr-PD} für zwei Bilder $x$ und $y$ berechnet sich dann zu $pd(x,y) = \frac{1}{HWC}
 ||\phi(x) - \phi(y)||_2^{2}$, wobei $\phi(x)$ die Aktivierung der drittletzten
 Schicht des VGG Netzes bei Eingabe $x$ ist und $H \times W \times C$ das Format
 der Aktivierung ist. Die Aktivierung einer tieferen
 Convolution Schicht eines Klassifikationsnetzes, wie sie hier verwendet wird,
 ermöglicht es Bilder auf die Unterschiede im Inhalt zu bewerten, wobei
 Unterschiede im Stil weitestgehend ignoriert werden
 \cite{johnson_perceptual_2016,pang2021image}. Daher ist diese Distanz als
 Metrik geeignet, um die erfolgreiche Übertragung des Bildinhaltes durch ein
 \gls{acr-I2I} GAN zu bewerten.