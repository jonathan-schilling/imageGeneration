
@article{liu_few-shot_2019,
	title = {Few-Shot Unsupervised Image-to-Image Translation},
	url = {http://arxiv.org/abs/1905.01723},
	abstract = {Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are speciﬁed, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at https://github.com/{NVlabs}/{FUNIT}.},
	journaltitle = {{arXiv}:1905.01723 [cs, stat]},
	author = {Liu, Ming-Yu and Huang, Xun and Mallya, Arun and Karras, Tero and Aila, Timo and Lehtinen, Jaakko and Kautz, Jan},
	urldate = {2022-02-24},
	date = {2019-09-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.01723},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Graphics, Computer Science - Multimedia, Statistics - Machine Learning},
	file = {Liu et al. - 2019 - Few-Shot Unsupervised Image-to-Image Translation.pdf:/Users/daniela/Zotero/storage/5NVT6IMX/Liu et al. - 2019 - Few-Shot Unsupervised Image-to-Image Translation.pdf:application/pdf}
}

@article{zhu_unpaired_2020,
	title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transﬁguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	journaltitle = {{arXiv}:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	urldate = {2022-02-24},
	date = {2020-08-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:/Users/daniela/Zotero/storage/UGCJZ5FF/Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf}
}

@inproceedings{heusel_gans_2017,
	title = {{GANs} Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	urldate = {2022-02-26},
	date = {2017},
	keywords = {evaluation},
	file = {Full Text PDF:/Users/daniela/Zotero/storage/YKIC53ZT/Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf}
}

@article{johnson_perceptual_2016,
	title = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
	url = {http://arxiv.org/abs/1603.08155},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	journaltitle = {{arXiv}:1603.08155 [cs]},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	urldate = {2022-03-15},
	date = {2016-03-26},
	eprinttype = {arxiv},
	eprint = {1603.08155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/daniela/Zotero/storage/5LXDUYVV/Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:application/pdf;arXiv.org Snapshot:/Users/daniela/Zotero/storage/7H8VAW6E/1603.html:text/html}
}

@article{ulyanov_instance_2016,
	title = {Instance Normalization: The Missing Ingredient for Fast Stylization},
	url = {https://arxiv.org/abs/1607.08022v3},
	doi = {10.48550/arXiv.1607.08022},
	shorttitle = {Instance Normalization},
	abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/{DmitryUlyanov}/texture\_nets. Full paper can be found at {arXiv}:1701.02096.},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	urldate = {2022-03-15},
	date = {2016-07-27},
	langid = {english},
	file = {Snapshot:/Users/daniela/Zotero/storage/ZX6AHG5Q/1607.html:text/html;Full Text PDF:/Users/daniela/Zotero/storage/P68PR2JS/Ulyanov et al. - 2016 - Instance Normalization The Missing Ingredient for.pdf:application/pdf}
}